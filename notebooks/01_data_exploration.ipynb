{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 1: Data Engineering & Exploratory Data Analysis\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook covers:\n",
    "1. Loading and merging all 9 Olist datasets\n",
    "2. Creating time-based train/val/test split\n",
    "3. Exploratory Data Analysis (on training set ONLY)\n",
    "4. Missing value analysis and imputation\n",
    "5. Outlier detection and handling\n",
    "6. Schema validation\n",
    "\n",
    "**Key Principles:**\n",
    "- Split BEFORE EDA to prevent data leakage\n",
    "- All statistics computed on training set only\n",
    "- Transformers fit on train, applied to val/test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Our custom modules\n",
    "from src.data_loader import (\n",
    "    load_raw_data, parse_date_columns, \n",
    "    create_time_based_split, save_split_ids,\n",
    "    merge_datasets, create_target_variables,\n",
    "    apply_split, save_splits_parquet,\n",
    "    load_splits_parquet\n",
    ")\n",
    "\n",
    "from src.eda import (\n",
    "    get_basic_stats, analyze_column_types,\n",
    "    analyze_classification_target, analyze_regression_target,\n",
    "    analyze_missing_values, detect_outliers_iqr,\n",
    "    analyze_correlations, analyze_temporal_patterns,\n",
    "    analyze_categorical\n",
    ")\n",
    "\n",
    "from src.data_quality import (\n",
    "    process_training_data, process_inference_data,\n",
    "    save_artifacts, validate_dataframe\n",
    ")\n",
    "\n",
    "# Settings\n",
    "pd.set_option('display.max_columns', 50)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Raw Data\n",
    "\n",
    "Loading all 9 CSV files from the Olist Brazilian E-Commerce dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all 9 datasets\n",
    "datasets = load_raw_data('../data/raw/')\n",
    "\n",
    "# Parse date columns\n",
    "datasets = parse_date_columns(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick look at the orders table (our main table)\n",
    "datasets['orders'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Time-Based Split (CRITICAL!)\n",
    "\n",
    "We split the data BEFORE any analysis to prevent data leakage.\n",
    "\n",
    "**Strategy:** Time-based split\n",
    "- Train: 70% oldest orders (Sept 2016 - April 2018)\n",
    "- Val: 15% middle orders (April - June 2018)\n",
    "- Test: 15% newest orders (June - Oct 2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create time-based split\n",
    "train_ids, val_ids, test_ids = create_time_based_split(\n",
    "    datasets['orders'],\n",
    "    train_ratio=0.70,\n",
    "    val_ratio=0.15,\n",
    "    test_ratio=0.15\n",
    ")\n",
    "\n",
    "# Verify no overlap\n",
    "print(f\"\\nNo overlap verification:\")\n",
    "print(f\"  Train n Val: {len(set(train_ids) & set(val_ids))} (should be 0)\")\n",
    "print(f\"  Train n Test: {len(set(train_ids) & set(test_ids))} (should be 0)\")\n",
    "print(f\"  Val n Test: {len(set(val_ids) & set(test_ids))} (should be 0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Merge All Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all datasets into one\n",
    "merged = merge_datasets(datasets)\n",
    "\n",
    "# Create target variables\n",
    "merged = create_target_variables(merged)\n",
    "\n",
    "print(f\"\\nFinal merged shape: {merged.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply splits\n",
    "train_df, val_df, test_df = apply_split(merged, train_ids, val_ids, test_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. EDA on Training Set Only\n",
    "\n",
    "**IMPORTANT:** From here on, we only analyze the training set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "stats = get_basic_stats(train_df, \"Training Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Target Variable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification target: is_satisfied\n",
    "clf_analysis = analyze_classification_target(train_df, 'is_satisfied')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression target: delivery_days\n",
    "reg_analysis = analyze_regression_target(train_df, 'delivery_days')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize target distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Classification target\n",
    "train_df['is_satisfied'].value_counts().plot(kind='bar', ax=axes[0], color=['salmon', 'lightgreen'])\n",
    "axes[0].set_title('Customer Satisfaction Distribution')\n",
    "axes[0].set_xlabel('Satisfied (1) vs Unsatisfied (0)')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Regression target\n",
    "train_df['delivery_days'].dropna().hist(bins=50, ax=axes[1], color='steelblue', edgecolor='white')\n",
    "axes[1].axvline(train_df['delivery_days'].median(), color='red', linestyle='--', label=f'Median: {train_df[\"delivery_days\"].median():.1f}')\n",
    "axes[1].set_title('Delivery Days Distribution')\n",
    "axes[1].set_xlabel('Days')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Missing Value Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze missing values\n",
    "missing_df = analyze_missing_values(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing values\n",
    "cols_with_missing = missing_df[missing_df['missing_count'] > 0].head(15)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(cols_with_missing['column'], cols_with_missing['missing_pct'], color='coral')\n",
    "plt.xlabel('Missing %')\n",
    "plt.title('Columns with Missing Values')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Temporal Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal patterns\n",
    "temporal = analyze_temporal_patterns(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Orders over time\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Monthly trend\n",
    "monthly = train_df.groupby(train_df['order_purchase_timestamp'].dt.to_period('M')).size()\n",
    "monthly.plot(ax=axes[0], marker='o', color='steelblue')\n",
    "axes[0].set_title('Orders Over Time (Monthly)')\n",
    "axes[0].set_xlabel('Month')\n",
    "axes[0].set_ylabel('Number of Orders')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Day of week\n",
    "dow_counts = train_df['order_purchase_timestamp'].dt.dayofweek.value_counts().sort_index()\n",
    "dow_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "dow_counts.index = dow_names\n",
    "dow_counts.plot(kind='bar', ax=axes[1], color='lightgreen', edgecolor='darkgreen')\n",
    "axes[1].set_title('Orders by Day of Week')\n",
    "axes[1].set_xlabel('Day')\n",
    "axes[1].set_ylabel('Number of Orders')\n",
    "axes[1].tick_params(axis='x', rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Geographic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer states\n",
    "state_analysis = analyze_categorical(train_df, 'customer_state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 states\n",
    "plt.figure(figsize=(10, 6))\n",
    "train_df['customer_state'].value_counts().head(10).plot(kind='bar', color='teal', edgecolor='white')\n",
    "plt.title('Orders by Customer State (Top 10)')\n",
    "plt.xlabel('State')\n",
    "plt.ylabel('Number of Orders')\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Product Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Product categories\n",
    "category_analysis = analyze_categorical(train_df, 'product_category_name_english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 15 categories\n",
    "plt.figure(figsize=(12, 6))\n",
    "train_df['product_category_name_english'].value_counts().head(15).plot(kind='barh', color='steelblue')\n",
    "plt.title('Top 15 Product Categories')\n",
    "plt.xlabel('Number of Orders')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlations with satisfaction\n",
    "is_satisfied_corr = analyze_correlations(train_df, target_col='is_satisfied')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlations with delivery time\n",
    "delivery_corr = analyze_correlations(train_df, target_col='delivery_days')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "numeric_cols = ['price', 'freight_value', 'payment_value', 'delivery_days', \n",
    "                'delivery_delay_days', 'is_late_delivery', 'review_score', 'is_satisfied']\n",
    "corr_matrix = train_df[numeric_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='RdBu_r', center=0, fmt='.2f', square=True)\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7 Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect outliers\n",
    "key_cols = ['price', 'freight_value', 'payment_value', 'delivery_days', 'product_weight_g']\n",
    "outliers_df = detect_outliers_iqr(train_df, columns=key_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots for key numerical features\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "for i, col in enumerate(['price', 'freight_value', 'delivery_days', 'product_weight_g']):\n",
    "    train_df[col].dropna().plot(kind='box', ax=axes[i])\n",
    "    axes[i].set_title(f'{col}')\n",
    "\n",
    "plt.suptitle('Box Plots - Outlier Visualization', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Processing\n",
    "\n",
    "Apply missing value imputation and outlier handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process training data (fits transformers)\n",
    "train_processed, artifacts = process_training_data(train_df)\n",
    "\n",
    "# Save artifacts\n",
    "save_artifacts(artifacts, '../models/data_processing_artifacts.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process val and test using training artifacts\n",
    "val_processed = process_inference_data(val_df, artifacts)\n",
    "test_processed = process_inference_data(test_df, artifacts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate all datasets\n",
    "validate_dataframe(train_processed, 'Train')\n",
    "validate_dataframe(val_processed, 'Val')\n",
    "validate_dataframe(test_processed, 'Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed datasets\n",
    "train_processed.to_parquet('../data/processed/train_processed.parquet', index=False)\n",
    "val_processed.to_parquet('../data/processed/val_processed.parquet', index=False)\n",
    "test_processed.to_parquet('../data/processed/test_processed.parquet', index=False)\n",
    "\n",
    "print(\"Saved all processed datasets!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "**Target Variables:**\n",
    "- Classification: 72.5% satisfied, 27.5% unsatisfied (reasonably balanced)\n",
    "- Regression: Mean 13.7 days delivery, heavily right-skewed\n",
    "\n",
    "**Key Predictors of Satisfaction:**\n",
    "- Late delivery is the #1 driver of dissatisfaction (-0.30 correlation)\n",
    "- Delivery time strongly affects satisfaction (-0.29 correlation)\n",
    "\n",
    "**Geographic Insights:**\n",
    "- Sao Paulo (SP) accounts for 40% of orders\n",
    "- RJ and MG are #2 and #3\n",
    "\n",
    "**Product Insights:**\n",
    "- bed_bath_table, sports_leisure, furniture_decor are top categories\n",
    "- Credit card is dominant payment method (76%)\n",
    "\n",
    "### Files Created:\n",
    "- `data/splits/train.parquet`, `val.parquet`, `test.parquet`\n",
    "- `data/processed/train_processed.parquet`, etc.\n",
    "- `models/data_processing_artifacts.json`\n",
    "- `data/processed/eda_statistics.json`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps: Day 2\n",
    "\n",
    "In the next notebook, we will:\n",
    "1. Create sklearn pipelines for feature engineering\n",
    "2. Build RFM, temporal, geographic features\n",
    "3. Implement customer segmentation (clustering)\n",
    "4. Add cluster labels as features for supervised learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
