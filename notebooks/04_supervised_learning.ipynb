{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Supervised Learning\n",
    "\n",
    "**Day 3: Part 1 - Classification (Customer Satisfaction Prediction)**\n",
    "\n",
    "## Objectives\n",
    "1. Load featured data from Day 2\n",
    "2. Define feature sets (avoiding data leakage)\n",
    "3. Build preprocessing pipeline\n",
    "4. Establish baseline (DummyClassifier)\n",
    "5. Train and compare classification models:\n",
    "   - Logistic Regression\n",
    "   - Decision Tree\n",
    "   - Random Forest\n",
    "   - LightGBM (with hyperparameter tuning)\n",
    "6. SHAP interpretability analysis\n",
    "7. Save best model and artifacts\n",
    "\n",
    "## Key Principles\n",
    "- **Baselines FIRST** - Always establish dummy baselines before training real models\n",
    "- **Cross-validation** - Use StratifiedKFold for robust performance estimates\n",
    "- **No data leakage** - Exclude review_score and derived features\n",
    "- **Class imbalance** - Handle with class_weight='balanced'\n",
    "- **Primary metric** - ROC-AUC (threshold-independent, handles imbalance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# sklearn\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, classification_report, confusion_matrix,\n",
    "    roc_curve\n",
    ")\n",
    "\n",
    "# LightGBM\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# SHAP\n",
    "import shap\n",
    "\n",
    "# Joblib for saving\n",
    "import joblib\n",
    "\n",
    "print(\"Imports complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 50)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Create plots directory\n",
    "Path('../models/plots').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Random state: {RANDOM_STATE}\")\n",
    "print(\"Settings configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load featured data from Day 2\n",
    "train = pd.read_parquet('../data/processed/train_featured.parquet')\n",
    "val = pd.read_parquet('../data/processed/val_featured.parquet')\n",
    "test = pd.read_parquet('../data/processed/test_featured.parquet')\n",
    "\n",
    "print(\"Data loaded:\")\n",
    "print(f\"  Train: {train.shape[0]:,} rows x {train.shape[1]} columns\")\n",
    "print(f\"  Val:   {val.shape[0]:,} rows x {val.shape[1]} columns\")\n",
    "print(f\"  Test:  {test.shape[0]:,} rows x {test.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and merge cluster labels (optional enhancement)\n",
    "try:\n",
    "    clustered = pd.read_parquet('../models/customer_segments_clustered.parquet')\n",
    "    cluster_cols = ['customer_unique_id', 'cluster_id', 'customer_segment']\n",
    "    \n",
    "    # Merge if not already present\n",
    "    if 'cluster_id' not in train.columns:\n",
    "        train = train.merge(clustered[cluster_cols], on='customer_unique_id', how='left')\n",
    "        train['cluster_id'] = train['cluster_id'].fillna(-1).astype(int)\n",
    "        train['customer_segment'] = train['customer_segment'].fillna('unknown')\n",
    "        \n",
    "        val = val.merge(clustered[cluster_cols], on='customer_unique_id', how='left')\n",
    "        val['cluster_id'] = val['cluster_id'].fillna(-1).astype(int)\n",
    "        val['customer_segment'] = val['customer_segment'].fillna('unknown')\n",
    "        \n",
    "        test = test.merge(clustered[cluster_cols], on='customer_unique_id', how='left')\n",
    "        test['cluster_id'] = test['cluster_id'].fillna(-1).astype(int)\n",
    "        test['customer_segment'] = test['customer_segment'].fillna('unknown')\n",
    "        \n",
    "        print(\"Merged cluster labels from customer_segments_clustered.parquet\")\n",
    "    else:\n",
    "        print(\"Cluster labels already present\")\n",
    "        \n",
    "    print(f\"\\nCluster distribution (train):\")\n",
    "    print(train['customer_segment'].value_counts())\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not load cluster labels: {e}\")\n",
    "    print(\"Proceeding without cluster features.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify target distribution\n",
    "print(\"=\" * 50)\n",
    "print(\"TARGET DISTRIBUTION: is_satisfied\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for name, df in [('Train', train), ('Val', val), ('Test', test)]:\n",
    "    satisfied = df['is_satisfied'].mean()\n",
    "    unsatisfied = 1 - satisfied\n",
    "    print(f\"{name}: {satisfied:.1%} satisfied, {unsatisfied:.1%} unsatisfied (n={len(df):,})\")\n",
    "\n",
    "# Imbalance ratio\n",
    "train_satisfied = train['is_satisfied'].sum()\n",
    "train_unsatisfied = len(train) - train_satisfied\n",
    "imbalance_ratio = train_satisfied / train_unsatisfied\n",
    "print(f\"\\nImbalance ratio (train): {imbalance_ratio:.2f}:1 (satisfied:unsatisfied)\")\n",
    "print(\"-> Moderate imbalance, will use class_weight='balanced'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Feature Sets\n",
    "\n",
    "**Critical: Avoid Data Leakage**\n",
    "\n",
    "Must EXCLUDE:\n",
    "- `review_score` - Target is derived from this\n",
    "- `is_satisfied` - Target variable\n",
    "- ID columns - Not predictive\n",
    "- Timestamp columns - Use derived features instead\n",
    "\n",
    "Safe to USE (delivery happens before review):\n",
    "- Delivery features: `delivery_days`, `is_late_delivery`, `delivery_delay_days`\n",
    "- Review text features: `review_sentiment_polarity`, `review_word_count`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature columns\n",
    "NUMERICAL_FEATURES = [\n",
    "    # Delivery (key predictors!)\n",
    "    'delivery_days', 'delivery_delay_days',\n",
    "    \n",
    "    # Price/Value\n",
    "    'price', 'freight_value', 'payment_value', 'payment_installments',\n",
    "    'freight_ratio', 'price_vs_category_zscore', 'payment_per_installment',\n",
    "    \n",
    "    # Product\n",
    "    'product_weight_g', 'product_volume_cm3', 'product_photos_qty',\n",
    "    \n",
    "    # Geographic\n",
    "    'seller_customer_distance_km',\n",
    "    \n",
    "    # NLP (review text features - written after delivery)\n",
    "    'review_text_length', 'review_word_count', 'review_caps_ratio',\n",
    "    'review_sentiment_polarity', 'review_sentiment_subjectivity',\n",
    "    \n",
    "    # Temporal (cyclical encoding)\n",
    "    'order_hour_sin', 'order_hour_cos',\n",
    "    'order_dayofweek_sin', 'order_dayofweek_cos',\n",
    "    \n",
    "    # RFM (customer-level)\n",
    "    'recency', 'frequency', 'monetary', 'avg_order_value',\n",
    "]\n",
    "\n",
    "CATEGORICAL_FEATURES = [\n",
    "    'customer_state', 'seller_state',\n",
    "    'customer_region', 'seller_region',\n",
    "    'product_category_name_english', 'payment_type',\n",
    "]\n",
    "\n",
    "BINARY_FEATURES = [\n",
    "    'is_weekend', 'is_month_start', 'is_month_end',\n",
    "    'is_same_state', 'is_full_payment', 'is_high_installment',\n",
    "    'has_review_comment', 'is_late_delivery',\n",
    "]\n",
    "\n",
    "# Add cluster features if available\n",
    "if 'customer_segment' in train.columns:\n",
    "    CATEGORICAL_FEATURES.append('customer_segment')\n",
    "    print(\"Added customer_segment to categorical features\")\n",
    "\n",
    "ALL_FEATURES = NUMERICAL_FEATURES + CATEGORICAL_FEATURES + BINARY_FEATURES\n",
    "\n",
    "print(f\"\\nFeature counts:\")\n",
    "print(f\"  Numerical: {len(NUMERICAL_FEATURES)}\")\n",
    "print(f\"  Categorical: {len(CATEGORICAL_FEATURES)}\")\n",
    "print(f\"  Binary: {len(BINARY_FEATURES)}\")\n",
    "print(f\"  Total: {len(ALL_FEATURES)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X, y\n",
    "X_train = train[ALL_FEATURES].copy()\n",
    "y_train = train['is_satisfied'].copy()\n",
    "\n",
    "X_val = val[ALL_FEATURES].copy()\n",
    "y_val = val['is_satisfied'].copy()\n",
    "\n",
    "X_test = test[ALL_FEATURES].copy()\n",
    "y_test = test['is_satisfied'].copy()\n",
    "\n",
    "print(\"Feature matrices created:\")\n",
    "print(f\"  X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"  X_val: {X_val.shape}, y_val: {y_val.shape}\")\n",
    "print(f\"  X_test: {X_test.shape}, y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values\n",
    "print(\"Missing values in X_train:\")\n",
    "missing = X_train.isnull().sum()\n",
    "missing_cols = missing[missing > 0].sort_values(ascending=False)\n",
    "\n",
    "if len(missing_cols) > 0:\n",
    "    print(missing_cols)\n",
    "    print(f\"\\n-> Will be handled by SimpleImputer in preprocessing pipeline\")\n",
    "else:\n",
    "    print(\"  No missing values!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocessing pipeline\n",
    "numerical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()),\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='unknown')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False)),\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numerical_transformer, NUMERICAL_FEATURES),\n",
    "    ('cat', categorical_transformer, CATEGORICAL_FEATURES),\n",
    "    ('bin', 'passthrough', BINARY_FEATURES),\n",
    "], remainder='drop')\n",
    "\n",
    "print(\"Preprocessing pipeline created:\")\n",
    "print(f\"  Numerical: {len(NUMERICAL_FEATURES)} features -> impute(median) + scale\")\n",
    "print(f\"  Categorical: {len(CATEGORICAL_FEATURES)} features -> impute + one-hot encode\")\n",
    "print(f\"  Binary: {len(BINARY_FEATURES)} features -> passthrough\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test preprocessing pipeline\n",
    "print(\"Testing preprocessing pipeline...\")\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_val_processed = preprocessor.transform(X_val)\n",
    "\n",
    "print(f\"\\nProcessed shapes:\")\n",
    "print(f\"  X_train: {X_train.shape} -> {X_train_processed.shape}\")\n",
    "print(f\"  X_val: {X_val.shape} -> {X_val_processed.shape}\")\n",
    "print(f\"\\nOne-hot encoding expanded {len(CATEGORICAL_FEATURES)} categorical features to {X_train_processed.shape[1] - len(NUMERICAL_FEATURES) - len(BINARY_FEATURES)} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Baseline Model (DummyClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DummyClassifier baseline\n",
    "print(\"=\" * 60)\n",
    "print(\"BASELINE: DummyClassifier (most_frequent)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "dummy = DummyClassifier(strategy='most_frequent', random_state=RANDOM_STATE)\n",
    "dummy.fit(X_train_processed, y_train)\n",
    "\n",
    "y_pred_dummy = dummy.predict(X_val_processed)\n",
    "\n",
    "baseline_accuracy = accuracy_score(y_val, y_pred_dummy)\n",
    "baseline_roc_auc = 0.5  # Random by definition for most_frequent strategy\n",
    "\n",
    "print(f\"\\nValidation Metrics:\")\n",
    "print(f\"  Accuracy: {baseline_accuracy:.4f}\")\n",
    "print(f\"  ROC-AUC:  {baseline_roc_auc:.4f} (random baseline)\")\n",
    "print(f\"\\n-> Any model must beat ROC-AUC > 0.50 to add value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store baseline metrics\n",
    "baseline_metrics = {\n",
    "    'name': 'Dummy (Baseline)',\n",
    "    'accuracy': baseline_accuracy,\n",
    "    'precision': 0.0,\n",
    "    'recall': 0.0,\n",
    "    'f1': 0.0,\n",
    "    'roc_auc': baseline_roc_auc,\n",
    "}\n",
    "\n",
    "# Initialize results list\n",
    "all_metrics = [baseline_metrics]\n",
    "trained_models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation function\n",
    "def evaluate_model(model, X, y, name):\n",
    "    \"\"\"Evaluate classifier and return metrics dict.\"\"\"\n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    # Get probabilities for ROC-AUC\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        y_proba = model.predict_proba(X)[:, 1]\n",
    "    else:\n",
    "        y_proba = y_pred\n",
    "    \n",
    "    return {\n",
    "        'name': name,\n",
    "        'accuracy': accuracy_score(y, y_pred),\n",
    "        'precision': precision_score(y, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y, y_pred, zero_division=0),\n",
    "        'f1': f1_score(y, y_pred, zero_division=0),\n",
    "        'roc_auc': roc_auc_score(y, y_proba),\n",
    "    }\n",
    "\n",
    "print(\"Evaluation function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model 1: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"MODEL 1: Logistic Regression\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "log_reg_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(\n",
    "        class_weight='balanced',\n",
    "        max_iter=1000,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1,\n",
    "    ))\n",
    "])\n",
    "\n",
    "print(\"Training Logistic Regression...\")\n",
    "log_reg_pipeline.fit(X_train, y_train)\n",
    "\n",
    "logreg_metrics = evaluate_model(log_reg_pipeline, X_val, y_val, 'Logistic Regression')\n",
    "print(f\"\\nValidation Metrics:\")\n",
    "print(f\"  Accuracy: {logreg_metrics['accuracy']:.4f}\")\n",
    "print(f\"  ROC-AUC:  {logreg_metrics['roc_auc']:.4f}\")\n",
    "print(f\"  F1:       {logreg_metrics['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation\n",
    "print(\"Running 5-fold cross-validation...\")\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "cv_scores = cross_val_score(log_reg_pipeline, X_train, y_train, cv=cv, scoring='roc_auc', n_jobs=-1)\n",
    "\n",
    "print(f\"CV ROC-AUC: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "logreg_metrics['cv_roc_auc_mean'] = cv_scores.mean()\n",
    "logreg_metrics['cv_roc_auc_std'] = cv_scores.std()\n",
    "\n",
    "all_metrics.append(logreg_metrics)\n",
    "trained_models['Logistic Regression'] = log_reg_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model 2: Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"MODEL 2: Decision Tree\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "dt_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', DecisionTreeClassifier(\n",
    "        max_depth=10,\n",
    "        min_samples_split=20,\n",
    "        min_samples_leaf=10,\n",
    "        class_weight='balanced',\n",
    "        random_state=RANDOM_STATE,\n",
    "    ))\n",
    "])\n",
    "\n",
    "print(\"Training Decision Tree...\")\n",
    "dt_pipeline.fit(X_train, y_train)\n",
    "\n",
    "dt_metrics = evaluate_model(dt_pipeline, X_val, y_val, 'Decision Tree')\n",
    "print(f\"\\nValidation Metrics:\")\n",
    "print(f\"  Accuracy: {dt_metrics['accuracy']:.4f}\")\n",
    "print(f\"  ROC-AUC:  {dt_metrics['roc_auc']:.4f}\")\n",
    "print(f\"  F1:       {dt_metrics['f1']:.4f}\")\n",
    "\n",
    "all_metrics.append(dt_metrics)\n",
    "trained_models['Decision Tree'] = dt_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model 3: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"MODEL 3: Random Forest\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "rf_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=15,\n",
    "        min_samples_split=10,\n",
    "        min_samples_leaf=5,\n",
    "        class_weight='balanced',\n",
    "        n_jobs=-1,\n",
    "        random_state=RANDOM_STATE,\n",
    "    ))\n",
    "])\n",
    "\n",
    "print(\"Training Random Forest (100 trees)...\")\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "\n",
    "rf_metrics = evaluate_model(rf_pipeline, X_val, y_val, 'Random Forest')\n",
    "print(f\"\\nValidation Metrics:\")\n",
    "print(f\"  Accuracy: {rf_metrics['accuracy']:.4f}\")\n",
    "print(f\"  ROC-AUC:  {rf_metrics['roc_auc']:.4f}\")\n",
    "print(f\"  F1:       {rf_metrics['f1']:.4f}\")\n",
    "\n",
    "all_metrics.append(rf_metrics)\n",
    "trained_models['Random Forest'] = rf_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model 4: LightGBM (Base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"MODEL 4: LightGBM (Base)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "lgbm_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LGBMClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=10,\n",
    "        learning_rate=0.1,\n",
    "        class_weight='balanced',\n",
    "        n_jobs=-1,\n",
    "        random_state=RANDOM_STATE,\n",
    "        verbose=-1,\n",
    "    ))\n",
    "])\n",
    "\n",
    "print(\"Training LightGBM (base model)...\")\n",
    "lgbm_pipeline.fit(X_train, y_train)\n",
    "\n",
    "lgbm_metrics = evaluate_model(lgbm_pipeline, X_val, y_val, 'LightGBM (base)')\n",
    "print(f\"\\nValidation Metrics:\")\n",
    "print(f\"  Accuracy: {lgbm_metrics['accuracy']:.4f}\")\n",
    "print(f\"  ROC-AUC:  {lgbm_metrics['roc_auc']:.4f}\")\n",
    "print(f\"  F1:       {lgbm_metrics['f1']:.4f}\")\n",
    "\n",
    "all_metrics.append(lgbm_metrics)\n",
    "trained_models['LightGBM (base)'] = lgbm_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. LightGBM Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"HYPERPARAMETER TUNING: LightGBM\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [100, 200, 300],\n",
    "    'classifier__max_depth': [5, 10, 15],\n",
    "    'classifier__learning_rate': [0.01, 0.05, 0.1],\n",
    "    'classifier__num_leaves': [31, 50, 100],\n",
    "    'classifier__min_child_samples': [20, 50, 100],\n",
    "    'classifier__subsample': [0.8, 1.0],\n",
    "    'classifier__colsample_bytree': [0.8, 1.0],\n",
    "}\n",
    "\n",
    "# Calculate total combinations\n",
    "total_combinations = 1\n",
    "for v in param_grid.values():\n",
    "    total_combinations *= len(v)\n",
    "print(f\"Total parameter combinations: {total_combinations}\")\n",
    "print(f\"Will sample 30 combinations with RandomizedSearchCV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create base pipeline for tuning\n",
    "lgbm_tune_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LGBMClassifier(\n",
    "        class_weight='balanced',\n",
    "        n_jobs=-1,\n",
    "        random_state=RANDOM_STATE,\n",
    "        verbose=-1,\n",
    "    ))\n",
    "])\n",
    "\n",
    "# RandomizedSearchCV\n",
    "print(\"\\nRunning RandomizedSearchCV (this may take a few minutes)...\")\n",
    "search = RandomizedSearchCV(\n",
    "    lgbm_tune_pipeline,\n",
    "    param_grid,\n",
    "    n_iter=30,\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE),\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    random_state=RANDOM_STATE,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "search.fit(X_train, y_train)\n",
    "print(\"\\nTuning complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model results\n",
    "print(\"=\" * 60)\n",
    "print(\"BEST MODEL: LightGBM (Tuned)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nBest CV ROC-AUC: {search.best_score_:.4f}\")\n",
    "print(f\"\\nBest Parameters:\")\n",
    "for param, value in search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "# Evaluate on validation set\n",
    "best_lgbm = search.best_estimator_\n",
    "lgbm_tuned_metrics = evaluate_model(best_lgbm, X_val, y_val, 'LightGBM (tuned)')\n",
    "\n",
    "print(f\"\\nValidation Metrics:\")\n",
    "print(f\"  Accuracy: {lgbm_tuned_metrics['accuracy']:.4f}\")\n",
    "print(f\"  ROC-AUC:  {lgbm_tuned_metrics['roc_auc']:.4f}\")\n",
    "print(f\"  F1:       {lgbm_tuned_metrics['f1']:.4f}\")\n",
    "\n",
    "lgbm_tuned_metrics['cv_roc_auc'] = search.best_score_\n",
    "lgbm_tuned_metrics['best_params'] = search.best_params_\n",
    "\n",
    "all_metrics.append(lgbm_tuned_metrics)\n",
    "trained_models['LightGBM (tuned)'] = best_lgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame(all_metrics)\n",
    "comparison_df = comparison_df[['name', 'accuracy', 'precision', 'recall', 'f1', 'roc_auc']]\n",
    "comparison_df = comparison_df.sort_values('roc_auc', ascending=False)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MODEL COMPARISON (Validation Set)\")\n",
    "print(\"=\" * 70)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Save comparison\n",
    "comparison_df.to_csv('../models/classification_comparison.csv', index=False)\n",
    "print(\"\\n✓ Saved: models/classification_comparison.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curves\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Plot ROC curves for each model\n",
    "models_to_plot = [\n",
    "    ('Logistic Regression', log_reg_pipeline),\n",
    "    ('Decision Tree', dt_pipeline),\n",
    "    ('Random Forest', rf_pipeline),\n",
    "    ('LightGBM (tuned)', best_lgbm),\n",
    "]\n",
    "\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "\n",
    "for (name, model), color in zip(models_to_plot, colors):\n",
    "    y_proba = model.predict_proba(X_val)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_val, y_proba)\n",
    "    auc = roc_auc_score(y_val, y_proba)\n",
    "    ax.plot(fpr, tpr, label=f'{name} (AUC={auc:.3f})', color=color, linewidth=2)\n",
    "\n",
    "# Random baseline\n",
    "ax.plot([0, 1], [0, 1], 'k--', label='Random (AUC=0.500)', linewidth=1)\n",
    "\n",
    "ax.set_xlabel('False Positive Rate', fontsize=12)\n",
    "ax.set_ylabel('True Positive Rate', fontsize=12)\n",
    "ax.set_title('ROC Curves - Classification Models', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='lower right', fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../models/plots/classification_roc_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Saved: models/plots/classification_roc_curves.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix for best model\n",
    "y_pred_best = best_lgbm.predict(X_val)\n",
    "cm = confusion_matrix(y_val, y_pred_best)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "            xticklabels=['Unsatisfied', 'Satisfied'],\n",
    "            yticklabels=['Unsatisfied', 'Satisfied'],\n",
    "            annot_kws={'size': 14})\n",
    "ax.set_xlabel('Predicted', fontsize=12)\n",
    "ax.set_ylabel('Actual', fontsize=12)\n",
    "ax.set_title('Confusion Matrix - LightGBM (tuned)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../models/plots/classification_confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Saved: models/plots/classification_confusion_matrix.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report\n",
    "print(\"=\" * 60)\n",
    "print(\"CLASSIFICATION REPORT: LightGBM (tuned)\")\n",
    "print(\"=\" * 60)\n",
    "print(classification_report(y_val, y_pred_best, target_names=['Unsatisfied', 'Satisfied']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. SHAP Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"SHAP FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get the classifier from the pipeline\n",
    "classifier = best_lgbm.named_steps['classifier']\n",
    "\n",
    "# Transform validation data\n",
    "X_val_transformed = best_lgbm.named_steps['preprocessor'].transform(X_val)\n",
    "\n",
    "# Get feature names after preprocessing\n",
    "def get_feature_names(preprocessor, num_features, cat_features, bin_features):\n",
    "    \"\"\"Extract feature names after pipeline transformation.\"\"\"\n",
    "    names = []\n",
    "    \n",
    "    # Numerical features (same names after scaling)\n",
    "    names.extend(num_features)\n",
    "    \n",
    "    # Categorical features (one-hot encoded names)\n",
    "    try:\n",
    "        cat_encoder = preprocessor.named_transformers_['cat'].named_steps['encoder']\n",
    "        cat_feature_names = cat_encoder.get_feature_names_out(cat_features)\n",
    "        names.extend(cat_feature_names.tolist())\n",
    "    except Exception:\n",
    "        names.extend(cat_features)\n",
    "    \n",
    "    # Binary features (passthrough)\n",
    "    names.extend(bin_features)\n",
    "    \n",
    "    return names\n",
    "\n",
    "feature_names = get_feature_names(\n",
    "    best_lgbm.named_steps['preprocessor'],\n",
    "    NUMERICAL_FEATURES,\n",
    "    CATEGORICAL_FEATURES,\n",
    "    BINARY_FEATURES\n",
    ")\n",
    "\n",
    "print(f\"Total features after preprocessing: {len(feature_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SHAP explainer\n",
    "print(\"\\nComputing SHAP values (this may take a moment)...\")\n",
    "\n",
    "# Use a sample for faster computation if dataset is large\n",
    "sample_size = min(5000, len(X_val_transformed))\n",
    "sample_idx = np.random.choice(len(X_val_transformed), sample_size, replace=False)\n",
    "X_sample = X_val_transformed[sample_idx]\n",
    "\n",
    "explainer = shap.TreeExplainer(classifier)\n",
    "shap_values = explainer.shap_values(X_sample)\n",
    "\n",
    "print(f\"SHAP values computed for {sample_size} samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Summary Plot (beeswarm)\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# For binary classification, shap_values is a list [class_0, class_1]\n",
    "if isinstance(shap_values, list):\n",
    "    shap_vals = shap_values[1]  # Use class 1 (satisfied)\n",
    "else:\n",
    "    shap_vals = shap_values\n",
    "\n",
    "shap.summary_plot(shap_vals, X_sample, feature_names=feature_names, show=False, max_display=20)\n",
    "plt.title('SHAP Feature Importance - Customer Satisfaction\\n(Top 20 Features)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../models/plots/shap_classification_summary.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Saved: models/plots/shap_classification_summary.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Bar Plot (mean absolute values)\n",
    "plt.figure(figsize=(10, 8))\n",
    "shap.summary_plot(shap_vals, X_sample, feature_names=feature_names, \n",
    "                  plot_type='bar', show=False, max_display=20)\n",
    "plt.title('Mean |SHAP| - Feature Importance\\n(Top 20 Features)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../models/plots/shap_classification_bar.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Saved: models/plots/shap_classification_bar.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top features by SHAP importance\n",
    "shap_importance = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': np.abs(shap_vals).mean(axis=0)\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TOP 20 FEATURES BY SHAP IMPORTANCE\")\n",
    "print(\"=\" * 60)\n",
    "print(shap_importance.head(20).to_string(index=False))\n",
    "\n",
    "# Save SHAP importance\n",
    "shap_importance.to_csv('../models/shap_classification_importance.csv', index=False)\n",
    "print(\"\\n✓ Saved: models/shap_classification_importance.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Save Best Model & Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model\n",
    "best_classifier = best_lgbm\n",
    "best_clf_name = 'LightGBM (tuned)'\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SAVING BEST CLASSIFICATION MODEL\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nBest Model: {best_clf_name}\")\n",
    "print(f\"Validation ROC-AUC: {lgbm_tuned_metrics['roc_auc']:.4f}\")\n",
    "print(f\"Validation Accuracy: {lgbm_tuned_metrics['accuracy']:.4f}\")\n",
    "print(f\"Validation F1: {lgbm_tuned_metrics['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "joblib.dump(best_classifier, '../models/satisfaction_classifier.joblib')\n",
    "print(\"\\n✓ Saved: models/satisfaction_classifier.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save classification metadata\n",
    "clf_metadata = {\n",
    "    'model_name': best_clf_name,\n",
    "    'algorithm': 'LightGBM',\n",
    "    'best_params': {k.replace('classifier__', ''): v for k, v in search.best_params_.items()},\n",
    "    'features': {\n",
    "        'numerical': NUMERICAL_FEATURES,\n",
    "        'categorical': CATEGORICAL_FEATURES,\n",
    "        'binary': BINARY_FEATURES,\n",
    "        'total_after_encoding': len(feature_names),\n",
    "    },\n",
    "    'metrics': {\n",
    "        'validation': {\n",
    "            'accuracy': lgbm_tuned_metrics['accuracy'],\n",
    "            'precision': lgbm_tuned_metrics['precision'],\n",
    "            'recall': lgbm_tuned_metrics['recall'],\n",
    "            'f1': lgbm_tuned_metrics['f1'],\n",
    "            'roc_auc': lgbm_tuned_metrics['roc_auc'],\n",
    "        },\n",
    "        'cv_roc_auc': float(search.best_score_),\n",
    "    },\n",
    "    'class_distribution': {\n",
    "        'train': {\n",
    "            'satisfied': float(y_train.mean()),\n",
    "            'unsatisfied': float(1 - y_train.mean()),\n",
    "        },\n",
    "        'val': {\n",
    "            'satisfied': float(y_val.mean()),\n",
    "            'unsatisfied': float(1 - y_val.mean()),\n",
    "        },\n",
    "    },\n",
    "    'top_features': shap_importance.head(10).to_dict('records'),\n",
    "    'training_date': datetime.now().isoformat(),\n",
    "}\n",
    "\n",
    "with open('../models/classification_metadata.json', 'w') as f:\n",
    "    json.dump(clf_metadata, f, indent=2, default=str)\n",
    "\n",
    "print(\"✓ Saved: models/classification_metadata.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Part 1 Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PART 1: CLASSIFICATION COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\"\"\n",
    "TASK: Customer Satisfaction Prediction\n",
    "TARGET: is_satisfied (binary: review_score >= 4)\n",
    "\n",
    "BEST MODEL: {best_clf_name}\n",
    "─────────────────────────────────────────\n",
    "  Validation ROC-AUC:  {lgbm_tuned_metrics['roc_auc']:.4f}\n",
    "  Validation Accuracy: {lgbm_tuned_metrics['accuracy']:.4f}\n",
    "  Validation F1:       {lgbm_tuned_metrics['f1']:.4f}\n",
    "  Validation Precision:{lgbm_tuned_metrics['precision']:.4f}\n",
    "  Validation Recall:   {lgbm_tuned_metrics['recall']:.4f}\n",
    "  CV ROC-AUC:          {search.best_score_:.4f}\n",
    "\n",
    "IMPROVEMENT OVER BASELINE:\n",
    "─────────────────────────────────────────\n",
    "  ROC-AUC: {baseline_roc_auc:.4f} → {lgbm_tuned_metrics['roc_auc']:.4f} (+{lgbm_tuned_metrics['roc_auc'] - baseline_roc_auc:.4f})\n",
    "\n",
    "TOP 5 PREDICTORS (by SHAP):\n",
    "─────────────────────────────────────────\"\"\")\n",
    "\n",
    "for i, row in shap_importance.head(5).iterrows():\n",
    "    print(f\"  {shap_importance.head(5).index.get_loc(i)+1}. {row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "print(f\"\"\"\n",
    "ARTIFACTS SAVED:\n",
    "─────────────────────────────────────────\n",
    "  ✓ models/satisfaction_classifier.joblib\n",
    "  ✓ models/classification_metadata.json\n",
    "  ✓ models/classification_comparison.csv\n",
    "  ✓ models/shap_classification_importance.csv\n",
    "  ✓ models/plots/classification_roc_curves.png\n",
    "  ✓ models/plots/classification_confusion_matrix.png\n",
    "  ✓ models/plots/shap_classification_summary.png\n",
    "  ✓ models/plots/shap_classification_bar.png\n",
    "\n",
    "{'='*70}\n",
    "Ready for Part 2: Regression (Delivery Time Prediction)!\n",
    "{'='*70}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Regression (Delivery Time Prediction)\n",
    "\n",
    "*To be implemented in next session*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
