{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 02 - Feature Engineering\n",
    "\n",
    "**Day 2, Part 1: Feature Engineering**\n",
    "\n",
    "## Objectives\n",
    "1. Load processed data from Day 1\n",
    "2. Create RFM features (customer-level)\n",
    "3. Create temporal features (order-level)\n",
    "4. Create geographic features (order-level)\n",
    "5. Create product features (order-level)\n",
    "6. Create NLP features (order-level)\n",
    "7. Create payment features (order-level)\n",
    "8. Build sklearn preprocessing pipeline\n",
    "9. Save all artifacts\n",
    "\n",
    "## Key Principles\n",
    "- **Fit on training data ONLY** - Apply same transformations to val/test\n",
    "- **No data leakage** - Customer aggregations from training set only\n",
    "- **Reproducibility** - Save all artifacts for production use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Import our feature engineering module\n",
    "from src.feature_engineering import (\n",
    "    load_geolocation_data,\n",
    "    engineer_features,\n",
    "    save_feature_artifacts,\n",
    "    create_preprocessing_pipeline,\n",
    "    NUMERICAL_FEATURES,\n",
    "    CATEGORICAL_FEATURES,\n",
    "    BINARY_FEATURES,\n",
    "    RFM_FEATURES,\n",
    ")\n",
    "\n",
    "# Settings\n",
    "pd.set_option('display.max_columns', 50)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Step 1: Load Processed Data from Day 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data from Day 1\n",
    "data_dir = Path(\"../data/processed\")\n",
    "\n",
    "train_df = pd.read_parquet(data_dir / \"train_processed.parquet\")\n",
    "val_df = pd.read_parquet(data_dir / \"val_processed.parquet\")\n",
    "test_df = pd.read_parquet(data_dir / \"test_processed.parquet\")\n",
    "\n",
    "print(f\"Train: {train_df.shape[0]:,} rows × {train_df.shape[1]} columns\")\n",
    "print(f\"Val:   {val_df.shape[0]:,} rows × {val_df.shape[1]} columns\")\n",
    "print(f\"Test:  {test_df.shape[0]:,} rows × {test_df.shape[1]} columns\")\n",
    "\n",
    "# Load geolocation data\n",
    "geo_df = load_geolocation_data(\"../data/raw/olist_geolocation_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check date range (reference date for RFM)\n",
    "print(\"Date ranges:\")\n",
    "print(f\"Train: {train_df['order_purchase_timestamp'].min().date()} to {train_df['order_purchase_timestamp'].max().date()}\")\n",
    "print(f\"Val:   {val_df['order_purchase_timestamp'].min().date()} to {val_df['order_purchase_timestamp'].max().date()}\")\n",
    "print(f\"Test:  {test_df['order_purchase_timestamp'].min().date()} to {test_df['order_purchase_timestamp'].max().date()}\")\n",
    "\n",
    "reference_date = train_df['order_purchase_timestamp'].max()\n",
    "print(f\"\\nReference date for RFM: {reference_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original columns\n",
    "print(f\"Original columns ({len(train_df.columns)}):\")\n",
    "print(train_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Step 2: Run Feature Engineering Pipeline\n",
    "\n",
    "This will create all features:\n",
    "- **RFM**: recency, frequency, monetary, avg_order_value, monetary_per_day\n",
    "- **Temporal**: hour, dayofweek, month + cyclical encoding + binary flags\n",
    "- **Geographic**: distance_km, is_same_state, customer/seller regions\n",
    "- **Product**: volume, density, price ratios\n",
    "- **NLP**: text stats, sentiment\n",
    "- **Payment**: installment features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the complete feature engineering pipeline\n",
    "result = engineer_features(\n",
    "    train_df=train_df,\n",
    "    val_df=val_df,\n",
    "    test_df=test_df,\n",
    "    geo_df=geo_df,\n",
    "    reference_date=reference_date,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract results\n",
    "train_featured = result['train']\n",
    "val_featured = result['val']\n",
    "test_featured = result['test']\n",
    "customer_features = result['customer_features']\n",
    "category_stats = result['category_stats']\n",
    "\n",
    "print(f\"\\nFeatured data shapes:\")\n",
    "print(f\"Train: {train_featured.shape}\")\n",
    "print(f\"Val:   {val_featured.shape}\")\n",
    "print(f\"Test:  {test_featured.shape}\")\n",
    "print(f\"Customer features: {customer_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Step 3: Analyze New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New columns created\n",
    "original_cols = set(train_df.columns)\n",
    "new_cols = set(train_featured.columns) - original_cols\n",
    "\n",
    "print(f\"New features created: {len(new_cols)}\")\n",
    "print(\"\\nNew columns:\")\n",
    "for col in sorted(new_cols):\n",
    "    print(f\"  - {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RFM Features Summary\n",
    "print(\"=\" * 50)\n",
    "print(\"RFM FEATURES (Customer-Level)\")\n",
    "print(\"=\" * 50)\n",
    "print(customer_features[RFM_FEATURES].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize RFM distributions\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "for idx, col in enumerate(RFM_FEATURES):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    customer_features[col].hist(bins=50, ax=ax, edgecolor='black', alpha=0.7)\n",
    "    ax.set_title(f'{col}', fontsize=12)\n",
    "    ax.set_xlabel(col)\n",
    "    ax.set_ylabel('Frequency')\n",
    "\n",
    "# Remove empty subplot\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "plt.suptitle('RFM Feature Distributions', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal Features\n",
    "print(\"=\" * 50)\n",
    "print(\"TEMPORAL FEATURES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "temporal_cols = ['order_hour', 'order_dayofweek', 'order_month', 'is_weekend', \n",
    "                 'is_month_start', 'is_month_end', 'order_hour_sin', 'order_hour_cos']\n",
    "print(train_featured[temporal_cols].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize temporal patterns\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Hour distribution\n",
    "train_featured['order_hour'].value_counts().sort_index().plot(kind='bar', ax=axes[0], color='steelblue')\n",
    "axes[0].set_title('Orders by Hour', fontsize=12)\n",
    "axes[0].set_xlabel('Hour')\n",
    "axes[0].set_ylabel('Count')\n",
    "\n",
    "# Day of week\n",
    "days = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "day_counts = train_featured['order_dayofweek'].value_counts().sort_index()\n",
    "axes[1].bar(days, day_counts.values, color='coral')\n",
    "axes[1].set_title('Orders by Day of Week', fontsize=12)\n",
    "axes[1].set_xlabel('Day')\n",
    "axes[1].set_ylabel('Count')\n",
    "\n",
    "# Weekend vs Weekday\n",
    "weekend_counts = train_featured['is_weekend'].value_counts()\n",
    "axes[2].pie(weekend_counts.values, labels=['Weekday', 'Weekend'], autopct='%1.1f%%',\n",
    "            colors=['#66b3ff', '#ff9999'])\n",
    "axes[2].set_title('Weekend vs Weekday', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geographic Features\n",
    "print(\"=\" * 50)\n",
    "print(\"GEOGRAPHIC FEATURES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "geo_cols = ['seller_customer_distance_km', 'is_same_state', 'customer_region', 'seller_region']\n",
    "print(f\"Average distance: {train_featured['seller_customer_distance_km'].mean():.1f} km\")\n",
    "print(f\"Same state orders: {train_featured['is_same_state'].mean():.1%}\")\n",
    "print(f\"\\nCustomer regions:\\n{train_featured['customer_region'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Distance histogram\n",
    "train_featured['seller_customer_distance_km'].hist(bins=50, ax=axes[0], color='green', edgecolor='black', alpha=0.7)\n",
    "axes[0].set_title('Seller-Customer Distance Distribution', fontsize=12)\n",
    "axes[0].set_xlabel('Distance (km)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].axvline(train_featured['seller_customer_distance_km'].median(), color='red', linestyle='--', label='Median')\n",
    "axes[0].legend()\n",
    "\n",
    "# Region distribution\n",
    "train_featured['customer_region'].value_counts().plot(kind='bar', ax=axes[1], color='purple', edgecolor='black')\n",
    "axes[1].set_title('Orders by Customer Region', fontsize=12)\n",
    "axes[1].set_xlabel('Region')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].tick_params(axis='x', rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Product Features\n",
    "print(\"=\" * 50)\n",
    "print(\"PRODUCT FEATURES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "product_cols = ['product_volume_cm3', 'product_density', 'price_per_kg', 'freight_ratio', \n",
    "                'price_vs_category_mean', 'price_vs_category_zscore']\n",
    "print(train_featured[product_cols].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP Features\n",
    "print(\"=\" * 50)\n",
    "print(\"NLP FEATURES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "nlp_cols = ['has_review_comment', 'review_text_length', 'review_word_count', \n",
    "            'review_exclamation_count', 'review_caps_ratio',\n",
    "            'review_sentiment_polarity', 'review_sentiment_subjectivity']\n",
    "print(train_featured[nlp_cols].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment vs Review Score correlation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Sentiment by review score\n",
    "sentiment_by_score = train_featured.groupby('review_score')['review_sentiment_polarity'].mean()\n",
    "sentiment_by_score.plot(kind='bar', ax=axes[0], color='orange', edgecolor='black')\n",
    "axes[0].set_title('Average Sentiment Polarity by Review Score', fontsize=12)\n",
    "axes[0].set_xlabel('Review Score')\n",
    "axes[0].set_ylabel('Avg Sentiment Polarity')\n",
    "axes[0].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Word count by review score\n",
    "wordcount_by_score = train_featured.groupby('review_score')['review_word_count'].mean()\n",
    "wordcount_by_score.plot(kind='bar', ax=axes[1], color='teal', edgecolor='black')\n",
    "axes[1].set_title('Average Word Count by Review Score', fontsize=12)\n",
    "axes[1].set_xlabel('Review Score')\n",
    "axes[1].set_ylabel('Avg Word Count')\n",
    "axes[1].tick_params(axis='x', rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Payment Features\n",
    "print(\"=\" * 50)\n",
    "print(\"PAYMENT FEATURES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "payment_cols = ['payment_per_installment', 'is_full_payment', 'is_high_installment']\n",
    "print(train_featured[payment_cols].describe())\n",
    "\n",
    "print(f\"\\nFull payment rate: {train_featured['is_full_payment'].mean():.1%}\")\n",
    "print(f\"High installment rate (>=6): {train_featured['is_high_installment'].mean():.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## Step 4: Feature Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numerical features for correlation\n",
    "numerical_for_corr = [\n",
    "    'price', 'freight_value', 'payment_value',\n",
    "    'product_weight_g', 'product_volume_cm3', 'seller_customer_distance_km',\n",
    "    'freight_ratio', 'review_sentiment_polarity', 'review_word_count',\n",
    "    'recency', 'frequency', 'monetary', 'delivery_days', 'is_satisfied'\n",
    "]\n",
    "\n",
    "# Filter to columns that exist\n",
    "numerical_for_corr = [c for c in numerical_for_corr if c in train_featured.columns]\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr_matrix = train_featured[numerical_for_corr].corr()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 10))\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='RdBu_r', center=0,\n",
    "            fmt='.2f', square=True, linewidths=0.5)\n",
    "plt.title('Feature Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlations with target variables\n",
    "print(\"=\" * 50)\n",
    "print(\"CORRELATIONS WITH TARGETS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Classification target: is_satisfied\n",
    "print(\"\\nCorrelations with is_satisfied (classification target):\")\n",
    "corr_with_satisfied = train_featured[numerical_for_corr].corrwith(train_featured['is_satisfied']).sort_values(ascending=False)\n",
    "print(corr_with_satisfied.head(10))\n",
    "\n",
    "# Regression target: delivery_days\n",
    "print(\"\\nCorrelations with delivery_days (regression target):\")\n",
    "corr_with_delivery = train_featured[numerical_for_corr].corrwith(train_featured['delivery_days']).sort_values(ascending=False)\n",
    "print(corr_with_delivery.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## Step 5: Customer-Level Features Analysis (for Clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer features summary\n",
    "print(\"=\" * 50)\n",
    "print(\"CUSTOMER-LEVEL FEATURES (for Clustering)\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nTotal customers: {len(customer_features):,}\")\n",
    "print(\"\\nFeature statistics:\")\n",
    "print(customer_features.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer feature correlations\n",
    "cluster_features = ['recency', 'frequency', 'monetary', 'avg_order_value', \n",
    "                   'avg_review_score', 'avg_delivery_days', 'late_delivery_rate']\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "corr = customer_features[cluster_features].corr()\n",
    "sns.heatmap(corr, annot=True, cmap='RdBu_r', center=0, fmt='.2f', square=True)\n",
    "plt.title('Customer Features Correlation (for Clustering)', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "## Step 6: Build and Test Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocessing pipeline\n",
    "preprocessor = create_preprocessing_pipeline(\n",
    "    numerical_features=NUMERICAL_FEATURES,\n",
    "    categorical_features=CATEGORICAL_FEATURES,\n",
    "    binary_features=BINARY_FEATURES\n",
    ")\n",
    "\n",
    "print(f\"\\nNumerical features: {len(NUMERICAL_FEATURES)}\")\n",
    "print(f\"Categorical features: {len(CATEGORICAL_FEATURES)}\")\n",
    "print(f\"Binary features: {len(BINARY_FEATURES)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit on training data\n",
    "X_train = train_featured[NUMERICAL_FEATURES + CATEGORICAL_FEATURES + BINARY_FEATURES].copy()\n",
    "\n",
    "# Handle any remaining NaN values\n",
    "print(f\"NaN values before fit: {X_train.isnull().sum().sum()}\")\n",
    "\n",
    "# Fit and transform\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "print(f\"\\nTransformed shape: {X_train_transformed.shape}\")\n",
    "print(f\"Original features: {len(NUMERICAL_FEATURES) + len(CATEGORICAL_FEATURES) + len(BINARY_FEATURES)}\")\n",
    "print(f\"After one-hot encoding: {X_train_transformed.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names after transformation\n",
    "from src.feature_engineering import get_feature_names_from_pipeline\n",
    "\n",
    "feature_names = get_feature_names_from_pipeline(\n",
    "    preprocessor, NUMERICAL_FEATURES, CATEGORICAL_FEATURES, BINARY_FEATURES\n",
    ")\n",
    "\n",
    "print(f\"Total feature names: {len(feature_names)}\")\n",
    "print(f\"\\nSample feature names:\")\n",
    "print(feature_names[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-32",
   "metadata": {},
   "source": [
    "## Step 7: Save All Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all artifacts\n",
    "save_feature_artifacts(\n",
    "    result=result,\n",
    "    output_dir=\"../data/processed\",\n",
    "    models_dir=\"../models\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save preprocessing pipeline\n",
    "import joblib\n",
    "\n",
    "joblib.dump(preprocessor, '../models/feature_pipeline.joblib')\n",
    "print(\"✓ Saved preprocessing pipeline to models/feature_pipeline.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify saved files\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"SAVED FILES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Data files\n",
    "data_dir = Path(\"../data/processed\")\n",
    "for f in sorted(data_dir.glob(\"*.parquet\")):\n",
    "    size_mb = f.stat().st_size / 1024 / 1024\n",
    "    print(f\"  {f.name}: {size_mb:.2f} MB\")\n",
    "\n",
    "# Model files\n",
    "models_dir = Path(\"../models\")\n",
    "for f in sorted(models_dir.glob(\"*\")):\n",
    "    if f.is_file():\n",
    "        size_kb = f.stat().st_size / 1024\n",
    "        print(f\"  {f.name}: {size_kb:.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-36",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Features Created\n",
    "\n",
    "| Category | Count | Examples |\n",
    "|----------|-------|----------|\n",
    "| RFM | 5 | recency, frequency, monetary, avg_order_value, monetary_per_day |\n",
    "| Temporal | 12 | order_hour, is_weekend, hour_sin, hour_cos |\n",
    "| Geographic | 6 | seller_customer_distance_km, is_same_state, customer_region |\n",
    "| Product | 6 | product_volume_cm3, freight_ratio, price_vs_category_zscore |\n",
    "| NLP | 8 | review_sentiment_polarity, review_word_count, review_caps_ratio |\n",
    "| Payment | 3 | payment_per_installment, is_full_payment, is_high_installment |\n",
    "\n",
    "### Artifacts Saved\n",
    "\n",
    "- `data/processed/train_featured.parquet` - Training data with all features\n",
    "- `data/processed/val_featured.parquet` - Validation data with all features  \n",
    "- `data/processed/test_featured.parquet` - Test data with all features\n",
    "- `data/processed/customer_segments.parquet` - Customer-level features for clustering\n",
    "- `models/feature_pipeline.joblib` - sklearn preprocessing pipeline\n",
    "- `models/category_stats.csv` - Category price statistics\n",
    "- `models/feature_names.json` - Feature column names\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Continue to **Day 2, Part 2: Unsupervised Learning (Clustering)** in notebook `03_unsupervised_learning.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-37",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DAY 2, PART 1: FEATURE ENGINEERING COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\"\"\n",
    "✓ Created {len(new_cols)} new features\n",
    "✓ Saved train/val/test featured data\n",
    "✓ Saved customer-level features for clustering\n",
    "✓ Saved preprocessing pipeline\n",
    "\n",
    "Ready for Part 2: Clustering!\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
